{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNBvSfmQV9cY"
      },
      "source": [
        "The size of the entire dataset is roughly 343.51 GB. This will obviously not work on Google Colab given Google Colab's disk space. One way we can work around this is by mounting the GCS of the competition data to Google Colab.\n",
        "\n",
        "Relevant links:\n",
        "\n",
        "- [RSNA 2022 Cervical Spine Fracture Detection](https://www.kaggle.com/competitions/rsna-2022-cervical-spine-fracture-detection/overview)\n",
        "\n",
        "- [How to access kaggle competition data without using Google Drive / colab disk space](https://slash-z.com/google-colab-mount-kaggle-competition-dataset/)\n",
        "\n",
        "\n",
        "All credits go to the original author [Qishen Ha](https://www.kaggle.com/code/haqishen/rsna-2022-1st-place-solution-train-stage1/notebook)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdEyFNHiaDOv"
      },
      "outputs": [],
      "source": [
        "## Upload kaggle.json & Save kaggle.json to ~/.kaggle/\n",
        "!mkdir ~/.kaggle && mv kaggle.json ~/.kaggle/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klXVj9A1V1ij"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQ01of-bZn2x"
      },
      "outputs": [],
      "source": [
        "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-`lsb_release -c -s` main\" | sudo tee /etc/apt/sources.list.d/gcsfuse.list\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
        "!apt-get -y -q update\n",
        "!apt-get -y -q install gcsfuse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYbYnnBMaMFf"
      },
      "outputs": [],
      "source": [
        "!mkdir -p tmp\n",
        "!gcsfuse --implicit-dirs --limit-bytes-per-sec -1 --limit-ops-per-sec -1 \"ADD_GCS_PATH\" tmp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldRDI6D3aO0A"
      },
      "source": [
        "We should now have access to the dataset in tmp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMHHaS-8aUIe"
      },
      "source": [
        "Extra datasets & files we need."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OW3PTgkxaMDa"
      },
      "outputs": [],
      "source": [
        "### Extra files\n",
        "! kaggle datasets download -d boliu0/covn3d-same\n",
        "! kaggle datasets download -d haqishen/pylibjpeg140py3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhZep4tpaMBD"
      },
      "outputs": [],
      "source": [
        "! unzip covn3d-same.zip && unzip pylibjpeg140py3.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeTUpyD8aaTK"
      },
      "outputs": [],
      "source": [
        "!pip -q install monai\n",
        "!pip -q install segmentation-models-pytorch==0.2.1\n",
        "!pip -q install pylibjpeg-1.4.0-py3-none-any.whl\n",
        "!pip -q install python_gdcm-3.0.17.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
        "!pip -q install pydicom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KZUOxjJaaRc"
      },
      "outputs": [],
      "source": [
        "DEBUG = False\n",
        "\n",
        "import os\n",
        "import sys\n",
        "sys.path = [\n",
        "    '/content/covn3d-same',\n",
        "] + sys.path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3ZyP5DHaaPV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import gc\n",
        "import ast\n",
        "import cv2\n",
        "import time\n",
        "import timm\n",
        "import pickle\n",
        "import random\n",
        "import pydicom\n",
        "import argparse\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from glob import glob\n",
        "import nibabel as nib\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import albumentations\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "import segmentation_models_pytorch as smp\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.cuda.amp as amp\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "from monai.transforms import Resize\n",
        "import  monai.transforms as transforms\n",
        "\n",
        "%matplotlib inline\n",
        "rcParams['figure.figsize'] = 20, 8\n",
        "device = torch.device('cuda')\n",
        "torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1mR4TUOag-S"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Yh41BXfaaNR"
      },
      "outputs": [],
      "source": [
        "kernel_type = 'timm3d_res18d_unet4b_128_128_128_dsv2_flip12_shift333p7_gd1p5_bs4_lr3e4_20x50ep'\n",
        "load_kernel = None\n",
        "load_last = True\n",
        "n_blocks = 4\n",
        "n_folds = 5\n",
        "backbone = 'resnet18d'\n",
        "\n",
        "image_sizes = [128, 128, 128]\n",
        "R = Resize(image_sizes)\n",
        "\n",
        "init_lr = 3e-3\n",
        "batch_size = 4\n",
        "drop_rate = 0.\n",
        "drop_path_rate = 0.\n",
        "loss_weights = [1, 1]\n",
        "p_mixup = 0.1\n",
        "\n",
        "data_dir = './tmp/'\n",
        "use_amp = True\n",
        "num_workers = 4\n",
        "out_dim = 7\n",
        "\n",
        "n_epochs = 1000\n",
        "\n",
        "log_dir = './logs'\n",
        "model_dir = './models'\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "os.makedirs(model_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OucTe6Aqajtf"
      },
      "outputs": [],
      "source": [
        "transforms_train = transforms.Compose([\n",
        "    transforms.RandFlipd(keys=[\"image\", \"mask\"], prob=0.5, spatial_axis=1),\n",
        "    transforms.RandFlipd(keys=[\"image\", \"mask\"], prob=0.5, spatial_axis=2),\n",
        "    transforms.RandAffined(keys=[\"image\", \"mask\"], translate_range=[int(x*y) for x, y in zip(image_sizes, [0.3, 0.3, 0.3])], padding_mode='zeros', prob=0.7),\n",
        "    transforms.RandGridDistortiond(keys=(\"image\", \"mask\"), prob=0.5, distort_limit=(-0.01, 0.01), mode=\"nearest\"),    \n",
        "])\n",
        "\n",
        "transforms_valid = transforms.Compose([\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsMFX0RQajq9"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv(os.path.join(data_dir, 'train.csv'))\n",
        "\n",
        "mask_files = os.listdir(f'{data_dir}/segmentations')\n",
        "df_mask = pd.DataFrame({\n",
        "    'mask_file': mask_files,\n",
        "})\n",
        "df_mask['StudyInstanceUID'] = df_mask['mask_file'].apply(lambda x: x[:-4])\n",
        "df_mask['mask_file'] = df_mask['mask_file'].apply(lambda x: os.path.join(data_dir, 'segmentations', x))\n",
        "df = df_train.merge(df_mask, on='StudyInstanceUID', how='left')\n",
        "df['image_folder'] = df['StudyInstanceUID'].apply(lambda x: os.path.join(data_dir, 'train_images', x))\n",
        "df['mask_file'].fillna('', inplace=True)\n",
        "\n",
        "df_seg = df.query('mask_file != \"\"').reset_index(drop=True)\n",
        "\n",
        "kf = KFold(5)\n",
        "df_seg['fold'] = -1\n",
        "for fold, (train_idx, valid_idx) in enumerate(kf.split(df_seg, df_seg)):\n",
        "    df_seg.loc[valid_idx, 'fold'] = fold\n",
        "\n",
        "df_seg.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IP83RycJaor4"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CFXbSfyaapPp"
      },
      "outputs": [],
      "source": [
        "revert_list = [\n",
        "    '1.2.826.0.1.3680043.1363',\n",
        "    '1.2.826.0.1.3680043.20120',\n",
        "    '1.2.826.0.1.3680043.2243',\n",
        "    '1.2.826.0.1.3680043.24606',\n",
        "    '1.2.826.0.1.3680043.32071'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "k8iJw0OVapc8"
      },
      "outputs": [],
      "source": [
        "def load_dicom(path):\n",
        "    dicom = pydicom.read_file(path)\n",
        "    data = dicom.pixel_array\n",
        "    data = cv2.resize(data, (image_sizes[0], image_sizes[1]), interpolation = cv2.INTER_LINEAR)\n",
        "    return data\n",
        "\n",
        "\n",
        "def load_dicom_line_par(path):\n",
        "\n",
        "    t_paths = sorted(glob(os.path.join(path, \"*\")),\n",
        "       key=lambda x: int(x.split('/')[-1].split(\".\")[0]))\n",
        "\n",
        "    n_scans = len(t_paths)\n",
        "    indices = np.quantile(list(range(n_scans)), np.linspace(0., 1., image_sizes[2])).round().astype(int)\n",
        "    t_paths = [t_paths[i] for i in indices]\n",
        "\n",
        "    images = []\n",
        "    for filename in t_paths:\n",
        "        images.append(load_dicom(filename))\n",
        "    images = np.stack(images, -1)\n",
        "    \n",
        "    images = images - np.min(images)\n",
        "    images = images / (np.max(images) + 1e-4)\n",
        "    images = (images * 255).astype(np.uint8)\n",
        "\n",
        "    return images\n",
        "\n",
        "\n",
        "def load_sample(row, has_mask=True):\n",
        "\n",
        "    image = load_dicom_line_par(row.image_folder)\n",
        "    if image.ndim < 4:\n",
        "        image = np.expand_dims(image, 0).repeat(3, 0)  # to 3ch\n",
        "\n",
        "    if has_mask:\n",
        "        mask_org = nib.load(row.mask_file).get_fdata()\n",
        "        shape = mask_org.shape\n",
        "        mask_org = mask_org.transpose(1, 0, 2)[::-1, :, ::-1]  # (d, w, h)\n",
        "        mask = np.zeros((7, shape[0], shape[1], shape[2]))\n",
        "        for cid in range(7):\n",
        "            mask[cid] = (mask_org == (cid+1))\n",
        "        mask = mask.astype(np.uint8) * 255\n",
        "        mask = R(mask).numpy()\n",
        "        \n",
        "        return image, mask\n",
        "    else:\n",
        "        return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWyLdwtjasBh"
      },
      "outputs": [],
      "source": [
        "## Used for saving preprocesssed data to .npy\n",
        "class SaveToDiskSEGDataset(Dataset):\n",
        "    def __init__(self, df, mode, transform):\n",
        "\n",
        "        self.df = df.reset_index()\n",
        "        self.mode = mode\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.df.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.df.iloc[index]\n",
        "\n",
        "        ### not using cache\n",
        "        image, mask = load_sample(row, has_mask=True)\n",
        "    \n",
        "        if row.StudyInstanceUID in revert_list:\n",
        "            mask = mask[:, :, :, ::-1]\n",
        "\n",
        "        res = self.transform({'image':image, 'mask':mask})\n",
        "        image = res['image'] / 255.\n",
        "        mask = res['mask']\n",
        "        mask = (mask > 127).astype(np.float32)\n",
        "\n",
        "        image, mask = torch.tensor(image).float(), torch.tensor(mask).float()\n",
        "\n",
        "        return row.StudyInstanceUID, image, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQbikGsOa01l"
      },
      "outputs": [],
      "source": [
        "rcParams['figure.figsize'] = 20,8\n",
        "\n",
        "df_show = df_seg\n",
        "dataset_show = SaveToDiskSEGDataset(df_show, 'train', transform=transforms_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZS_SRJDa43S"
      },
      "source": [
        "# Save to .npy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCF070t5a0zq"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "data = Path('data')\n",
        "data.mkdir(exist_ok=True)\n",
        "\n",
        "train_image_path_data = Path('data/train_images_npy')\n",
        "train_image_path_data.mkdir(exist_ok=True)\n",
        "\n",
        "segmentation_path_data = Path('data/segmentations_npy')\n",
        "segmentation_path_data.mkdir(exist_ok=True)\n",
        "\n",
        "for step, (study_instance_uid, image, mask) in tqdm(enumerate(dataset_show), total=len(dataset_show)):\n",
        "  study_instance_uid = study_instance_uid\n",
        "  image = image.cpu().detach().numpy()\n",
        "  mask = mask.cpu().detach().numpy()\n",
        "\n",
        "  np.save(os.path.join(train_image_path_data, f'{study_instance_uid}'), image)\n",
        "  np.save(os.path.join(segmentation_path_data, f'{study_instance_uid}'), mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmoBXuNma0xk"
      },
      "outputs": [],
      "source": [
        "## zip data folder\n",
        "! zip -r data.zip data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-VdsFMvbAtp"
      },
      "outputs": [],
      "source": [
        "## Upload dataset to Kaggle datasets\n",
        "! kaggle datasets init -p data/\n",
        "! kaggle datasets create -p data/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
