{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rREoB99uSpsF"
      },
      "source": [
        "# Preprocessing data for [Stage 2 Type 1](https://www.kaggle.com/code/haqishen/rsna-2022-1st-place-solution-train-stage2-type1)\n",
        "\n",
        "All credit goes to the original author [Qishen Ha](https://www.kaggle.com/code/haqishen/rsna-2022-1st-place-solution-train-stage2-type1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcS8Aa3T0l2D"
      },
      "source": [
        "# Mount dataset onto Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QzbKjuG0DY_"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_YvPLmg0jeI",
        "outputId": "05ddbd67-6819-4777-ae9b-20c0a817eba2"
      },
      "outputs": [],
      "source": [
        "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-`lsb_release -c -s` main\" | sudo tee /etc/apt/sources.list.d/gcsfuse.list\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
        "!apt-get -y -q update\n",
        "!apt-get -y -q install gcsfuse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZnR884M0yht",
        "outputId": "a7193327-c6b8-449a-f911-e1a742337335"
      },
      "outputs": [],
      "source": [
        "!mkdir -p tmp\n",
        "!gcsfuse --implicit-dirs --limit-bytes-per-sec -1 --limit-ops-per-sec -1 \"ADD_GCS_PATH\" tmp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06-zCwsK02gH"
      },
      "outputs": [],
      "source": [
        "!mkdir ~/.kaggle && mv kaggle.json ~/.kaggle/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roNH-Drq08zd",
        "outputId": "cdae1d43-5040-47fe-8559-fc959be8e89c"
      },
      "outputs": [],
      "source": [
        "### Extra files\n",
        "! kaggle datasets download -d boliu0/covn3d-same\n",
        "! kaggle datasets download -d haqishen/pylibjpeg140py3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRHjTCdv0-_Y",
        "outputId": "af15c4bf-97ef-4755-f4df-a61ba7a5e768"
      },
      "outputs": [],
      "source": [
        "!unzip covn3d-same.zip && unzip pylibjpeg140py3.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hj40oDyL0_XA",
        "outputId": "df19f271-c1cd-4b11-cc73-c1d0cf8022b1"
      },
      "outputs": [],
      "source": [
        "!pip -q install monai\n",
        "!pip -q install segmentation-models-pytorch==0.2.1\n",
        "!pip -q install pylibjpeg-1.4.0-py3-none-any.whl\n",
        "!pip -q install python_gdcm-3.0.17.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
        "!pip -q install pydicom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thuh_CZc1FZd"
      },
      "outputs": [],
      "source": [
        "DEBUG = False\n",
        "\n",
        "import os\n",
        "import sys\n",
        "sys.path = [\n",
        "    '/content/covn3d-same',\n",
        "] + sys.path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdV8lR121FS2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import gc\n",
        "import ast\n",
        "import cv2\n",
        "import time\n",
        "import timm\n",
        "import pickle\n",
        "import random\n",
        "import pydicom\n",
        "import argparse\n",
        "import warnings\n",
        "import threading\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from glob import glob\n",
        "import nibabel as nib\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import albumentations\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "import segmentation_models_pytorch as smp\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.cuda.amp as amp\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "from monai.transforms import Resize\n",
        "import  monai.transforms as transforms\n",
        "\n",
        "%matplotlib inline\n",
        "rcParams['figure.figsize'] = 20, 8\n",
        "device = torch.device('cuda')\n",
        "torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhLz6Yd51rnO"
      },
      "outputs": [],
      "source": [
        "data_dir = '/content/tmp'\n",
        "image_size_seg = (128, 128, 128)\n",
        "msk_size = image_size_seg[0]\n",
        "image_size_cls = 224\n",
        "n_slice_per_c = 15\n",
        "n_ch = 5\n",
        "\n",
        "drop_rate = 0.\n",
        "drop_path_rate = 0.\n",
        "loss_weights = [1, 1]\n",
        "p_mixup = 0.1\n",
        "batch_size_seg = 1\n",
        "batch_size = 4\n",
        "num_workers = 2\n",
        "\n",
        "n_blocks = 4\n",
        "n_folds = 5\n",
        "backbone = 'resnet18d'\n",
        "out_dim = 7\n",
        "\n",
        "out_dir = './preprocess_stage2_type1'\n",
        "os.makedirs(out_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2M-BMR21w-m"
      },
      "outputs": [],
      "source": [
        "def load_dicom(path):\n",
        "    dicom = pydicom.read_file(path)\n",
        "    data = dicom.pixel_array\n",
        "    data = cv2.resize(data, (image_size_seg[0], image_size_seg[1]), interpolation = cv2.INTER_AREA)\n",
        "    return data\n",
        "\n",
        "\n",
        "def load_dicom_line_par(path):\n",
        "\n",
        "    t_paths = sorted(glob(os.path.join(path, \"*\")), key=lambda x: int(x.split('/')[-1].split(\".\")[0]))\n",
        "\n",
        "    n_scans = len(t_paths)\n",
        "    indices = np.quantile(list(range(n_scans)), np.linspace(0., 1., image_size_seg[2])).round().astype(int)\n",
        "    t_paths = [t_paths[i] for i in indices]\n",
        "\n",
        "    images = []\n",
        "    for filename in t_paths:\n",
        "        images.append(load_dicom(filename))\n",
        "    images = np.stack(images, -1)\n",
        "    \n",
        "    images = images - np.min(images)\n",
        "    images = images / (np.max(images) + 1e-4)\n",
        "    images = (images * 255).astype(np.uint8)\n",
        "\n",
        "    return images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "k7bDXpnI1y7v",
        "outputId": "6b6b7100-dcdf-4b0f-ba45-e70c3f703ff7"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv(os.path.join(data_dir, 'train.csv'))\n",
        "\n",
        "mask_files = os.listdir(f'{data_dir}/segmentations')\n",
        "df_mask = pd.DataFrame({\n",
        "    'mask_file': mask_files,\n",
        "})\n",
        "df_mask['StudyInstanceUID'] = df_mask['mask_file'].apply(lambda x: x[:-4])\n",
        "df_mask['mask_file'] = df_mask['mask_file'].apply(lambda x: os.path.join(data_dir, 'segmentations', x))\n",
        "df = df_train.merge(df_mask, on='StudyInstanceUID', how='left')\n",
        "df['image_folder'] = df['StudyInstanceUID'].apply(lambda x: os.path.join(data_dir, 'train_images', x))\n",
        "df['mask_file'].fillna('', inplace=True)\n",
        "\n",
        "kf = KFold(5)\n",
        "df['fold'] = -1\n",
        "for fold, (train_idx, valid_idx) in enumerate(kf.split(df, df)):\n",
        "    df.loc[valid_idx, 'fold'] = fold\n",
        "\n",
        "df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "pFQ4-Jp22J1z",
        "outputId": "a14ec2a6-fb29-4615-f8e8-5be47ddb3f62"
      },
      "outputs": [],
      "source": [
        "print(df.shape)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "qLsCtsC92O3Z",
        "outputId": "29a85570-49cb-4c7d-a08c-28227ec2675f"
      },
      "outputs": [],
      "source": [
        "sid = []\n",
        "cs = []\n",
        "label = []\n",
        "fold = []\n",
        "image_folder = []\n",
        "for _, row in df.iterrows():\n",
        "    for i in [1,2,3,4,5,6,7]:\n",
        "        sid.append(row.StudyInstanceUID)\n",
        "        image_folder.append(row.image_folder)\n",
        "        cs.append(i)\n",
        "        label.append(row[f'C{i}'])\n",
        "        fold.append(row.fold)\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'StudyInstanceUID': sid,\n",
        "    'c': cs,\n",
        "    'label': label,\n",
        "    'image_folder': image_folder,\n",
        "    'fold': fold\n",
        "})\n",
        "\n",
        "df.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3l34E4eRfT6"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oIPY-fq2TyR"
      },
      "outputs": [],
      "source": [
        "class SegDataset(Dataset):\n",
        "\n",
        "    def __init__(self, df):\n",
        "        self.df = df.reset_index()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.df.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.df.iloc[index]\n",
        "        \n",
        "        studyinstanceuid = row.StudyInstanceUID\n",
        "        c = row.c\n",
        "\n",
        "        image = load_dicom_line_par(row.image_folder)\n",
        "        if image.ndim < 4:\n",
        "            image = np.expand_dims(image, 0)\n",
        "        image = image.astype(np.float32).repeat(3, 0)  # to 3ch\n",
        "        image = image / 255.\n",
        "\n",
        "        ### using local cache\n",
        "        # image_file = os.path.join('./data/train_images_npy/', f'{row.StudyInstanceUID}.npy')\n",
        "        # image = np.load(image_file).astype(np.float32)\n",
        "\n",
        "        return {\n",
        "            \"StudyInstanceUID\": studyinstanceuid,\n",
        "            \"c\": c,\n",
        "            \"image\": torch.tensor(image).float()\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obFeAkkcRi4N"
      },
      "source": [
        "# Saving dataset to disk to use for caching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1BFv5YeRihT"
      },
      "outputs": [],
      "source": [
        "# from pathlib import Path\n",
        "\n",
        "# data = Path('data')\n",
        "# data.mkdir(exist_ok=True)\n",
        "\n",
        "# stage2_train_image_path_data = Path('data/stage2_train_images_npy')\n",
        "# stage2_train_image_path_data.mkdir(exist_ok=True)\n",
        "\n",
        "# for step, (batch) in tqdm(enumerate(dataset_seg), total=len(dataset_seg)):\n",
        "#   image = batch['image'].cpu().detach().numpy()\n",
        "#   study_instance_uid = batch['StudyInstanceUID'].pop()\n",
        "#   c = int(batch['c'].cpu().detach().numpy())\n",
        "\n",
        "#   np.save(os.path.join(stage2_train_image_path_data, f'{study_instance_uid}'), image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jVcCu7W2rlu"
      },
      "outputs": [],
      "source": [
        "dataset_seg = SegDataset(df)\n",
        "loader_seg = torch.utils.data.DataLoader(dataset_seg, batch_size=batch_size_seg, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "# check batch\n",
        "batch = dataset_seg[0]\n",
        "batch['StudyInstanceUID'], batch['c'], batch['image'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1Dt9nXL2tDO"
      },
      "outputs": [],
      "source": [
        "class TimmSegModel(nn.Module):\n",
        "    def __init__(self, backbone, segtype='unet', pretrained=False):\n",
        "        super(TimmSegModel, self).__init__()\n",
        "\n",
        "        self.encoder = timm.create_model(\n",
        "            backbone,\n",
        "            in_chans=3,\n",
        "            features_only=True,\n",
        "            drop_rate=drop_rate,\n",
        "            drop_path_rate=drop_path_rate,\n",
        "            pretrained=pretrained\n",
        "        )\n",
        "        g = self.encoder(torch.rand(1, 3, 64, 64))\n",
        "        encoder_channels = [1] + [_.shape[1] for _ in g]\n",
        "        decoder_channels = [256, 128, 64, 32, 16]\n",
        "        if segtype == 'unet':\n",
        "            self.decoder = smp.unet.decoder.UnetDecoder(\n",
        "                encoder_channels=encoder_channels[:n_blocks+1],\n",
        "                decoder_channels=decoder_channels[:n_blocks],\n",
        "                n_blocks=n_blocks,\n",
        "            )\n",
        "\n",
        "        self.segmentation_head = nn.Conv2d(decoder_channels[n_blocks-1], out_dim, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "\n",
        "    def forward(self,x):\n",
        "        global_features = [0] + self.encoder(x)[:n_blocks]\n",
        "        seg_features = self.decoder(*global_features)\n",
        "        seg_features = self.segmentation_head(seg_features)\n",
        "        return seg_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30nBKtHX2xl5",
        "outputId": "f7ccc2dd-ceca-4443-c5aa-5418ba374c45"
      },
      "outputs": [],
      "source": [
        "from timm.models.layers.conv2d_same import Conv2dSame\n",
        "from conv3d_same import Conv3dSame\n",
        "\n",
        "def convert_3d(module):\n",
        "\n",
        "    module_output = module\n",
        "    if isinstance(module, torch.nn.BatchNorm2d):\n",
        "        module_output = torch.nn.BatchNorm3d(\n",
        "            module.num_features,\n",
        "            module.eps,\n",
        "            module.momentum,\n",
        "            module.affine,\n",
        "            module.track_running_stats,\n",
        "        )\n",
        "        if module.affine:\n",
        "            with torch.no_grad():\n",
        "                module_output.weight = module.weight\n",
        "                module_output.bias = module.bias\n",
        "        module_output.running_mean = module.running_mean\n",
        "        module_output.running_var = module.running_var\n",
        "        module_output.num_batches_tracked = module.num_batches_tracked\n",
        "        if hasattr(module, \"qconfig\"):\n",
        "            module_output.qconfig = module.qconfig\n",
        "            \n",
        "    elif isinstance(module, Conv2dSame):\n",
        "        module_output = Conv3dSame(\n",
        "            in_channels=module.in_channels,\n",
        "            out_channels=module.out_channels,\n",
        "            kernel_size=module.kernel_size[0],\n",
        "            stride=module.stride[0],\n",
        "            padding=module.padding[0],\n",
        "            dilation=module.dilation[0],\n",
        "            groups=module.groups,\n",
        "            bias=module.bias is not None,\n",
        "        )\n",
        "        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n",
        "\n",
        "    elif isinstance(module, torch.nn.Conv2d):\n",
        "        module_output = torch.nn.Conv3d(\n",
        "            in_channels=module.in_channels,\n",
        "            out_channels=module.out_channels,\n",
        "            kernel_size=module.kernel_size[0],\n",
        "            stride=module.stride[0],\n",
        "            padding=module.padding[0],\n",
        "            dilation=module.dilation[0],\n",
        "            groups=module.groups,\n",
        "            bias=module.bias is not None,\n",
        "            padding_mode=module.padding_mode\n",
        "        )\n",
        "        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n",
        "\n",
        "    elif isinstance(module, torch.nn.MaxPool2d):\n",
        "        module_output = torch.nn.MaxPool3d(\n",
        "            kernel_size=module.kernel_size,\n",
        "            stride=module.stride,\n",
        "            padding=module.padding,\n",
        "            dilation=module.dilation,\n",
        "            ceil_mode=module.ceil_mode,\n",
        "        )\n",
        "    elif isinstance(module, torch.nn.AvgPool2d):\n",
        "        module_output = torch.nn.AvgPool3d(\n",
        "            kernel_size=module.kernel_size,\n",
        "            stride=module.stride,\n",
        "            padding=module.padding,\n",
        "            ceil_mode=module.ceil_mode,\n",
        "        )\n",
        "\n",
        "    for name, child in module.named_children():\n",
        "        module_output.add_module(\n",
        "            name, convert_3d(child)\n",
        "        )\n",
        "    del module\n",
        "\n",
        "    return module_output\n",
        "\n",
        "m = TimmSegModel(backbone)\n",
        "m = convert_3d(m)\n",
        "m(torch.rand(1, 3, 128,128,128)).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPKOYmM_3KrW"
      },
      "outputs": [],
      "source": [
        "def load_bone(msk, cid, t_paths, cropped_images):\n",
        "    n_scans = len(t_paths)\n",
        "    bone = []\n",
        "    try:\n",
        "        msk_b = msk[cid] > 0.2\n",
        "        msk_c = msk[cid] > 0.05\n",
        "\n",
        "        x = np.where(msk_b.sum(1).sum(1) > 0)[0]\n",
        "        y = np.where(msk_b.sum(0).sum(1) > 0)[0]\n",
        "        z = np.where(msk_b.sum(0).sum(0) > 0)[0]\n",
        "\n",
        "        if len(x) == 0 or len(y) == 0 or len(z) == 0:\n",
        "            x = np.where(msk_c.sum(1).sum(1) > 0)[0]\n",
        "            y = np.where(msk_c.sum(0).sum(1) > 0)[0]\n",
        "            z = np.where(msk_c.sum(0).sum(0) > 0)[0]\n",
        "\n",
        "        x1, x2 = max(0, x[0] - 1), min(msk.shape[1], x[-1] + 1)\n",
        "        y1, y2 = max(0, y[0] - 1), min(msk.shape[2], y[-1] + 1)\n",
        "        z1, z2 = max(0, z[0] - 1), min(msk.shape[3], z[-1] + 1)\n",
        "        zz1, zz2 = int(z1 / msk_size * n_scans), int(z2 / msk_size * n_scans)\n",
        "\n",
        "        inds = np.linspace(zz1 ,zz2-1 ,n_slice_per_c).astype(int)\n",
        "        inds_ = np.linspace(z1 ,z2-1 ,n_slice_per_c).astype(int)\n",
        "        for sid, (ind, ind_) in enumerate(zip(inds, inds_)):\n",
        "\n",
        "            msk_this = msk[cid, :, :, ind_]\n",
        "\n",
        "            images = []\n",
        "            for i in range(-n_ch//2+1, n_ch//2+1):\n",
        "                try:\n",
        "                    dicom = pydicom.read_file(t_paths[ind+i])\n",
        "                    images.append(dicom.pixel_array)\n",
        "                except:\n",
        "                    images.append(np.zeros((512, 512)))\n",
        "\n",
        "            data = np.stack(images, -1)\n",
        "            data = data - np.min(data)\n",
        "            data = data / (np.max(data) + 1e-4)\n",
        "            data = (data * 255).astype(np.uint8)\n",
        "            msk_this = msk_this[x1:x2, y1:y2]\n",
        "            xx1 = int(x1 / msk_size * data.shape[0])\n",
        "            xx2 = int(x2 / msk_size * data.shape[0])\n",
        "            yy1 = int(y1 / msk_size * data.shape[1])\n",
        "            yy2 = int(y2 / msk_size * data.shape[1])\n",
        "            data = data[xx1:xx2, yy1:yy2]\n",
        "            data = np.stack([cv2.resize(data[:, :, i], (image_size_cls, image_size_cls), interpolation = cv2.INTER_LINEAR) for i in range(n_ch)], -1)\n",
        "            msk_this = (msk_this * 255).astype(np.uint8)\n",
        "            msk_this = cv2.resize(msk_this, (image_size_cls, image_size_cls), interpolation = cv2.INTER_LINEAR)\n",
        "\n",
        "            data = np.concatenate([data, msk_this[:, :, np.newaxis]], -1)\n",
        "\n",
        "            bone.append(torch.tensor(data))\n",
        "\n",
        "    except:\n",
        "        for sid in range(n_slice_per_c):\n",
        "            bone.append(torch.ones((image_size_cls, image_size_cls, n_ch+1)).int())\n",
        "\n",
        "    cropped_images[cid] = torch.stack(bone, 0)\n",
        "\n",
        "\n",
        "def load_cropped_images(msk, image_folder, n_ch=n_ch):\n",
        "    ### debugging \n",
        "    # import pdb; pdb.set_trace()\n",
        "    t_paths = sorted(glob(os.path.join(image_folder, \"*\")), key=lambda x: int(x.split('/')[-1].split(\".\")[0]))\n",
        "    for cid in range(7):\n",
        "        threads[cid] = threading.Thread(target=load_bone, args=(msk, cid, t_paths, cropped_images))\n",
        "        threads[cid].start()\n",
        "    for cid in range(7):\n",
        "        threads[cid].join()\n",
        "\n",
        "    return torch.cat(cropped_images, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63nKoOrWIQ_n",
        "outputId": "fa4075b6-1095-4fdb-fc95-cf97a5d82d2e"
      },
      "outputs": [],
      "source": [
        "models_seg = []\n",
        "\n",
        "kernel_type = 'timm3d_res18d_unet4b_128_128_128_dsv2_flip12_shift333p7_gd1p5_bs4_lr3e4_20x50ep'\n",
        "backbone = 'resnet18d'\n",
        "model_dir_seg = './drive/MyDrive/models/'\n",
        "for fold in range(5):\n",
        "    model = TimmSegModel(backbone, pretrained=False)\n",
        "    model = convert_3d(model)\n",
        "    model = model.to(device)\n",
        "    load_model_file = os.path.join(model_dir_seg, f'timm3d_res18d_unet4b_128_128_128_dsv2_flip12_shift333p7_gd1p5_bs4_lr3e4_20x50ep_fold0_best.pth')\n",
        "    sd = torch.load(load_model_file)\n",
        "    if 'model_state_dict' in sd.keys():\n",
        "        sd = sd['model_state_dict']\n",
        "    sd = {k[7:] if k.startswith('module.') else k: sd[k] for k in sd.keys()}\n",
        "    model.load_state_dict(sd, strict=True)\n",
        "    model.eval()\n",
        "    models_seg.append(model)\n",
        "\n",
        "len(models_seg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzW1eewZQ6np"
      },
      "source": [
        "# Save preprocessed data to .npy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjeGjPm33ibc",
        "outputId": "e914f41d-7d57-4cda-a67a-ac88f8677c0f"
      },
      "outputs": [],
      "source": [
        "bar = tqdm(loader_seg)\n",
        "with torch.no_grad():\n",
        "    for batch_id, (batch) in enumerate(bar):\n",
        "        images = batch['image'].cuda()\n",
        "        study_instance_uid = batch['StudyInstanceUID'].pop()\n",
        "        cid = int(batch['c'].cpu().detach().numpy())\n",
        "\n",
        "        # SEG\n",
        "        pred_masks = []\n",
        "        for model in models_seg:\n",
        "            pmask = model(images).sigmoid()\n",
        "            pred_masks.append(pmask)\n",
        "        pred_masks = torch.stack(pred_masks, 0).mean(0).cpu().numpy()\n",
        "      \n",
        "        # Build cls input\n",
        "        cls_inp = []\n",
        "        threads = [None] * 7\n",
        "        cropped_images = [None] * 7\n",
        "\n",
        "        for i in range(pred_masks.shape[0]):\n",
        "            row = df.iloc[batch_id*batch_size_seg+i]\n",
        "            cropped_images = load_cropped_images(pred_masks[i], row.image_folder)\n",
        "            cls_inp.append(cropped_images.permute(0, 3, 1, 2).float() / 255.)\n",
        "        cls_inp = torch.stack(cls_inp, 0).to(device)  # (1, 105, 6, 224, 224)\n",
        "        \n",
        "        slices = []\n",
        "        for i in range(0, cls_inp.shape[1], 7):\n",
        "            ind = i // 7\n",
        "            filepath = os.path.join(out_dir, f'{study_instance_uid}_{cid}_{ind}.npy')\n",
        "            image_slice = cls_inp[:,i,:,:,:].squeeze(0).cpu().detach().numpy()\n",
        "            np.save(filepath, image_slice)\n",
        "        #     break\n",
        "        # break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psO5R7qWRAzd"
      },
      "source": [
        "# Check output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "ngN0fSRDM2ji",
        "outputId": "1e0d90bd-e13c-4126-f5fe-ce45f73f10b4"
      },
      "outputs": [],
      "source": [
        "img = np.load('./preprocess_stage2_type1/1.2.826.0.1.3680043.6200_1_0.npy')\n",
        "print(img.shape)\n",
        "single_img = img[0]\n",
        "plt.imshow(single_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDZb-GPwQvIo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
